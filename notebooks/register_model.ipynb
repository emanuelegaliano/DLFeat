{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18bfc5fc",
   "metadata": {},
   "source": [
    "# DLFeat - Custom Model Registration & Feature Extraction Examples\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. The DLFeat registration API for image, video, audio, text, and multimodal models.\n",
    "2. How to plug pre-trained PyTorch (and HF-style) models into DLFeat via the `register_*_model` functions.\n",
    "3. Extracting features with `DLFeatExtractor` and using them in simple scikit-learn classifiers.\n",
    "4. Running quick sanity checks and self-tests to validate registered models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e6b8544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/emanuelegaliano/DLFeat.git\n",
      "  Cloning https://github.com/emanuelegaliano/DLFeat.git to /tmp/pip-req-build-ll7xc951\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/emanuelegaliano/DLFeat.git /tmp/pip-req-build-ll7xc951\n",
      "  Resolved https://github.com/emanuelegaliano/DLFeat.git to commit 6b653b23a9d2fa3df55dc439f15721c9110e8479\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.9.0 in /home/manu/.local/lib/python3.14/site-packages (from dlfeat==0.6.0) (2.9.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/lib64/python3.14/site-packages (from dlfeat==0.6.0) (2.3.5)\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in /home/manu/.local/lib/python3.14/site-packages (from dlfeat==0.6.0) (1.7.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/lib64/python3.14/site-packages (from scikit-learn>=0.24.0->dlfeat==0.6.0) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/manu/.local/lib/python3.14/site-packages (from scikit-learn>=0.24.0->dlfeat==0.6.0) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/manu/.local/lib/python3.14/site-packages (from scikit-learn>=0.24.0->dlfeat==0.6.0) (3.6.0)\n",
      "Requirement already satisfied: filelock in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/manu/.local/lib/python3.14/site-packages (from sympy>=1.13.3->torch>=1.9.0->dlfeat==0.6.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib64/python3.14/site-packages (from jinja2->torch>=1.9.0->dlfeat==0.6.0) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/emanuelegaliano/DLFeat.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4801c712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlfeat file: /home/manu/.local/lib/python3.14/site-packages/dlfeat/__init__.py\n",
      "Has register_video_model? False\n",
      "Attrs containing 'register': []\n"
     ]
    }
   ],
   "source": [
    "import dlfeat, inspect\n",
    "print(\"dlfeat file:\", dlfeat.__file__)\n",
    "\n",
    "print(\"Has register_video_model?\", hasattr(dlfeat, \"register_video_model\"))\n",
    "print(\"Attrs containing 'register':\", [n for n in dir(dlfeat) if \"register\" in n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02738e",
   "metadata": {},
   "source": [
    "## Video custom model example\n",
    "\n",
    "This example shows how to define a tiny 3D CNN for videos, train it on a small synthetic dataset, register it in DLFeat with `register_video_model`, and then extract fixed-size feature vectors from raw `.mp4` files using `DLFeatExtractor`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3620ac36",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'register_video_model' from 'dlfeat' (/home/manu/.local/lib/python3.14/site-packages/dlfeat/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Import DLFeat (assumes dlfeat.py is on your PYTHONPATH or installed as a package)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdlfeat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_video_model, DLFeatExtractor\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Hyperparameters\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[1;32m     17\u001b[0m clip_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m          \u001b[38;5;66;03m# number of frames per clip\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'register_video_model' from 'dlfeat' (/home/manu/.local/lib/python3.14/site-packages/dlfeat/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Video custom model example: tiny 3D CNN + DLFeat registration\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "# Import DLFeat (assumes dlfeat.py is on your PYTHONPATH or installed as a package)\n",
    "from dlfeat import register_video_model, DLFeatExtractor\n",
    "\n",
    "# ---------------------------\n",
    "# Hyperparameters\n",
    "# ---------------------------\n",
    "clip_len = 8          # number of frames per clip\n",
    "frame_size = 64       # spatial resolution (H = W)\n",
    "feature_dim = 128     # output feature dimension for DLFeat\n",
    "num_classes = 4\n",
    "num_train_videos = 32\n",
    "num_val_videos = 8\n",
    "batch_size = 4\n",
    "epochs = 3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Create a tiny synthetic video dataset on disk\n",
    "# ---------------------------\n",
    "tmp_root = tempfile.mkdtemp(prefix=\"dlfeat_tiny_video_\")\n",
    "\n",
    "def make_random_video(path, num_frames=clip_len, size=frame_size):\n",
    "    \"\"\"Create a simple 'moving square' RGB video and save it as MP4.\"\"\"\n",
    "    video = torch.zeros(num_frames, size, size, 3, dtype=torch.uint8)  # [T, H, W, C]\n",
    "    for t in range(num_frames):\n",
    "        x0 = (t * 2) % (size - 8)\n",
    "        y0 = (t * 3) % (size - 8)\n",
    "        video[t, y0:y0+8, x0:x0+8, :] = torch.randint(\n",
    "            128, 255, (8, 8, 3), dtype=torch.uint8\n",
    "        )\n",
    "    torchvision.io.write_video(path, video, fps=8)\n",
    "\n",
    "def generate_split(n_samples, split_name):\n",
    "    paths, labels = [], []\n",
    "    for i in range(n_samples):\n",
    "        cls = i % num_classes\n",
    "        filename = os.path.join(tmp_root, f\"{split_name}_{i:03d}_class{cls}.mp4\")\n",
    "        make_random_video(filename)\n",
    "        paths.append(filename)\n",
    "        labels.append(cls)\n",
    "    return paths, torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "train_paths, train_labels = generate_split(num_train_videos, \"train\")\n",
    "val_paths, val_labels = generate_split(num_val_videos, \"val\")\n",
    "\n",
    "class VideoFileDataset(Dataset):\n",
    "    \"\"\"Simple dataset that loads .mp4 files and returns (C, T, H, W) tensors.\"\"\"\n",
    "    def __init__(self, paths, labels, clip_len, frame_size):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.clip_len = clip_len\n",
    "        self.frame_size = frame_size\n",
    "\n",
    "    def _load_video_tensor(self, path):\n",
    "        # video: [T, H, W, C]\n",
    "        video, _, _ = torchvision.io.read_video(path, pts_unit=\"sec\")\n",
    "        num_frames = video.size(0)\n",
    "\n",
    "        # Sample or pad to a fixed number of frames\n",
    "        if num_frames < self.clip_len:\n",
    "            pad = video[-1:].repeat(self.clip_len - num_frames, 1, 1, 1)\n",
    "            video = torch.cat([video, pad], dim=0)\n",
    "        else:\n",
    "            idx = torch.linspace(0, num_frames - 1, steps=self.clip_len).long()\n",
    "            video = video[idx]\n",
    "\n",
    "        # To [T, C, H, W]\n",
    "        video = video.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Resize frames and normalize to [0, 1]\n",
    "        video = torchvision.transforms.functional.resize(\n",
    "            video, [self.frame_size, self.frame_size], antialias=True\n",
    "        )\n",
    "        video = video.float() / 255.0\n",
    "\n",
    "        # Final shape [C, T, H, W]\n",
    "        return video.permute(1, 0, 2, 3)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_tensor = self._load_video_tensor(self.paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        return video_tensor, label\n",
    "\n",
    "train_ds = VideoFileDataset(train_paths, train_labels, clip_len, frame_size)\n",
    "val_ds   = VideoFileDataset(val_paths,   val_labels,   clip_len, frame_size)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Define a tiny 3D CNN backbone + classifier head\n",
    "# ---------------------------\n",
    "\n",
    "class TinyVideoBackbone(nn.Module):\n",
    "    \"\"\"Very small 3D CNN that maps (C, T, H, W) to a feature vector.\"\"\"\n",
    "    def __init__(self, feature_dim=feature_dim):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d((1, 2, 2)),           # pool only spatial dims\n",
    "            nn.Conv3d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool3d((None, 1, 1)) # keep time, pool H,W -> [B, 32, T, 1, 1]\n",
    "        )\n",
    "        self.proj = nn.Linear(32, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, T, H, W]\n",
    "        x = self.features(x)          # [B, 32, T, 1, 1]\n",
    "        x = x.mean(dim=2)             # temporal average -> [B, 32, 1, 1]\n",
    "        x = x.view(x.size(0), 32)     # [B, 32]\n",
    "        return self.proj(x)           # [B, feature_dim]\n",
    "\n",
    "class TinyVideoClassifier(nn.Module):\n",
    "    \"\"\"Backbone + linear head for quick supervised training.\"\"\"\n",
    "    def __init__(self, backbone, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        return self.head(feats)\n",
    "\n",
    "backbone = TinyVideoBackbone(feature_dim=feature_dim)\n",
    "model = TinyVideoClassifier(backbone, num_classes=num_classes).to(device)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Quick training loop (few epochs on tiny synthetic data)\n",
    "# ---------------------------\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for videos, labels in train_loader:\n",
    "        videos = videos.to(device)   # [B, C, T, H, W]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(videos)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * videos.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_ds)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - train loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Register the trained backbone with DLFeat\n",
    "# ---------------------------\n",
    "\n",
    "register_video_model(\n",
    "    model_name=\"tiny_video_cnn\",\n",
    "    dim=feature_dim,\n",
    "    model=backbone,      # pass the trained backbone instance\n",
    "    clip_len=clip_len,\n",
    "    input_size=frame_size,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Use DLFeatExtractor to get features for a list of video paths\n",
    "# ---------------------------\n",
    "\n",
    "extractor = DLFeatExtractor(\"tiny_video_cnn\", device=device)\n",
    "\n",
    "# Here we just reuse a few validation paths, but any list of .mp4 files works\n",
    "video_paths = val_paths[:4]\n",
    "features = extractor.transform(video_paths, batch_size=2)\n",
    "\n",
    "print(\"Extracted feature shape:\", features.shape)  # (N_videos, feature_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
