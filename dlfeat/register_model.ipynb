{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18bfc5fc",
   "metadata": {},
   "source": [
    "# DLFeat - Custom Model Registration & Feature Extraction Examples\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. The DLFeat registration API for image, video, audio, text, and multimodal models.\n",
    "2. How to plug pre-trained PyTorch (and HF-style) models into DLFeat via the `register_*_model` functions.\n",
    "3. Extracting features with `DLFeatExtractor` and using them in simple scikit-learn classifiers.\n",
    "4. Running quick sanity checks and self-tests to validate registered models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e6b8544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/emanuelegaliano/DLFeat.git\n",
      "  Cloning https://github.com/emanuelegaliano/DLFeat.git to /tmp/pip-req-build-rn3dqo4f\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/emanuelegaliano/DLFeat.git /tmp/pip-req-build-rn3dqo4f\n",
      "  Resolved https://github.com/emanuelegaliano/DLFeat.git to commit 294731d2925c93bdaa9515d7bb84b26f083ba040\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.9.0 in /home/manu/.local/lib/python3.14/site-packages (from dlfeat==0.6.0) (2.9.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/lib64/python3.14/site-packages (from dlfeat==0.6.0) (2.3.5)\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in /home/manu/.local/lib/python3.14/site-packages (from dlfeat==0.6.0) (1.7.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/lib64/python3.14/site-packages (from scikit-learn>=0.24.0->dlfeat==0.6.0) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/manu/.local/lib/python3.14/site-packages (from scikit-learn>=0.24.0->dlfeat==0.6.0) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/manu/.local/lib/python3.14/site-packages (from scikit-learn>=0.24.0->dlfeat==0.6.0) (3.6.0)\n",
      "Requirement already satisfied: filelock in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/manu/.local/lib/python3.14/site-packages (from torch>=1.9.0->dlfeat==0.6.0) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/manu/.local/lib/python3.14/site-packages (from sympy>=1.13.3->torch>=1.9.0->dlfeat==0.6.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib64/python3.14/site-packages (from jinja2->torch>=1.9.0->dlfeat==0.6.0) (3.0.2)\n",
      "Building wheels for collected packages: dlfeat\n",
      "  Building wheel for dlfeat (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dlfeat: filename=dlfeat-0.6.0-py3-none-any.whl size=18813 sha256=6905e26960a892ec1252699244b056b5f3be71343a6a7728c6ce17ccf62a52cc\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-77pm2p9_/wheels/fe/42/16/fb7f39fa13eccf41da6e563c4bb972ada2bc85b01e978ae5f1\n",
      "Successfully built dlfeat\n",
      "Installing collected packages: dlfeat\n",
      "Successfully installed dlfeat-0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/emanuelegaliano/DLFeat.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637a5e38",
   "metadata": {},
   "source": [
    "## Resolving imports of DLFeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "381a50e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: fix import\n",
    "from dlfeat_lib import (\n",
    "    DLFeatExtractor, \n",
    "    register_video_model, \n",
    "    register_image_model, \n",
    "    register_audio_model,\n",
    "    register_text_model,\n",
    "    register_multimodal_image_text_model,\n",
    "    register_multimodal_video_text_model\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02738e",
   "metadata": {},
   "source": [
    "## Video custom model example\n",
    "\n",
    "This example shows how to define a tiny 3D CNN for videos, train it on a small synthetic dataset, register it in DLFeat with `register_video_model`, and then extract fixed-size feature vectors from raw `.mp4` files using `DLFeatExtractor`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3620ac36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manu/.local/lib/python3.14/site-packages/torchvision/io/_video_deprecation_warning.py:9: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - train loss: 1.3968\n",
      "Epoch 2/3 - train loss: 1.3906\n",
      "Epoch 3/3 - train loss: 1.3890\n",
      "Extracted feature shape: (4, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manu/Documents/DLFeat/dlfeat/dlfeat_lib.py:921: UserWarning: DLFeat: Using manual basic per-frame transforms for tiny_video_cnn as weights.transforms() was not available/used for video_frame_transform.\n",
      "  warnings.warn(f\"DLFeat: Using manual basic per-frame transforms for {self.model_name} as weights.transforms() was not available/used for video_frame_transform.\")\n"
     ]
    }
   ],
   "source": [
    "# Video custom model example: tiny 3D CNN + DLFeat registration\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "# ---------------------------\n",
    "# Hyperparameters\n",
    "# ---------------------------\n",
    "clip_len = 8          # number of frames per clip\n",
    "frame_size = 64       # spatial resolution (H = W)\n",
    "feature_dim = 128     # output feature dimension for DLFeat\n",
    "num_classes = 4\n",
    "num_train_videos = 32\n",
    "num_val_videos = 8\n",
    "batch_size = 4\n",
    "epochs = 3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Create a tiny synthetic video dataset on disk\n",
    "# ---------------------------\n",
    "tmp_root = tempfile.mkdtemp(prefix=\"dlfeat_tiny_video_\")\n",
    "\n",
    "def make_random_video(path, num_frames=clip_len, size=frame_size):\n",
    "    \"\"\"Create a simple 'moving square' RGB video and save it as MP4.\"\"\"\n",
    "    video = torch.zeros(num_frames, size, size, 3, dtype=torch.uint8)  # [T, H, W, C]\n",
    "    for t in range(num_frames):\n",
    "        x0 = (t * 2) % (size - 8)\n",
    "        y0 = (t * 3) % (size - 8)\n",
    "        video[t, y0:y0+8, x0:x0+8, :] = torch.randint(\n",
    "            128, 255, (8, 8, 3), dtype=torch.uint8\n",
    "        )\n",
    "    torchvision.io.write_video(path, video, fps=8)\n",
    "\n",
    "def generate_split(n_samples, split_name):\n",
    "    paths, labels = [], []\n",
    "    for i in range(n_samples):\n",
    "        cls = i % num_classes\n",
    "        filename = os.path.join(tmp_root, f\"{split_name}_{i:03d}_class{cls}.mp4\")\n",
    "        make_random_video(filename)\n",
    "        paths.append(filename)\n",
    "        labels.append(cls)\n",
    "    return paths, torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "train_paths, train_labels = generate_split(num_train_videos, \"train\")\n",
    "val_paths, val_labels = generate_split(num_val_videos, \"val\")\n",
    "\n",
    "class VideoFileDataset(Dataset):\n",
    "    \"\"\"Simple dataset that loads .mp4 files and returns (C, T, H, W) tensors.\"\"\"\n",
    "    def __init__(self, paths, labels, clip_len, frame_size):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.clip_len = clip_len\n",
    "        self.frame_size = frame_size\n",
    "\n",
    "    def _load_video_tensor(self, path):\n",
    "        # video: [T, H, W, C]\n",
    "        video, _, _ = torchvision.io.read_video(path, pts_unit=\"sec\")\n",
    "        num_frames = video.size(0)\n",
    "\n",
    "        # Sample or pad to a fixed number of frames\n",
    "        if num_frames < self.clip_len:\n",
    "            pad = video[-1:].repeat(self.clip_len - num_frames, 1, 1, 1)\n",
    "            video = torch.cat([video, pad], dim=0)\n",
    "        else:\n",
    "            idx = torch.linspace(0, num_frames - 1, steps=self.clip_len).long()\n",
    "            video = video[idx]\n",
    "\n",
    "        # To [T, C, H, W]\n",
    "        video = video.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Resize frames and normalize to [0, 1]\n",
    "        video = torchvision.transforms.functional.resize( # type: ignore\n",
    "            video, [self.frame_size, self.frame_size], antialias=True\n",
    "        )\n",
    "        video = video.float() / 255.0\n",
    "\n",
    "        # Final shape [C, T, H, W]\n",
    "        return video.permute(1, 0, 2, 3)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_tensor = self._load_video_tensor(self.paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        return video_tensor, label\n",
    "\n",
    "train_ds = VideoFileDataset(train_paths, train_labels, clip_len, frame_size)\n",
    "val_ds   = VideoFileDataset(val_paths,   val_labels,   clip_len, frame_size)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Define a tiny 3D CNN backbone + classifier head\n",
    "# ---------------------------\n",
    "\n",
    "class TinyVideoBackbone(nn.Module):\n",
    "    \"\"\"Very small 3D CNN that maps (C, T, H, W) to a feature vector.\"\"\"\n",
    "    def __init__(self, feature_dim=feature_dim):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d((1, 2, 2)),           # pool only spatial dims\n",
    "            nn.Conv3d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool3d((None, 1, 1)) # keep time, pool H,W -> [B, 32, T, 1, 1]\n",
    "        )\n",
    "        self.proj = nn.Linear(32, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, T, H, W]\n",
    "        x = self.features(x)          # [B, 32, T, 1, 1]\n",
    "        x = x.mean(dim=2)             # temporal average -> [B, 32, 1, 1]\n",
    "        x = x.view(x.size(0), 32)     # [B, 32]\n",
    "        return self.proj(x)           # [B, feature_dim]\n",
    "\n",
    "class TinyVideoClassifier(nn.Module):\n",
    "    \"\"\"Backbone + linear head for quick supervised training.\"\"\"\n",
    "    def __init__(self, backbone, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        return self.head(feats)\n",
    "\n",
    "backbone = TinyVideoBackbone(feature_dim=feature_dim)\n",
    "model = TinyVideoClassifier(backbone, num_classes=num_classes).to(device)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Quick training loop (few epochs on tiny synthetic data)\n",
    "# ---------------------------\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for videos, labels in train_loader:\n",
    "        videos = videos.to(device)   # [B, C, T, H, W]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(videos)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * videos.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_ds)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - train loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Register the trained backbone with DLFeat\n",
    "# ---------------------------\n",
    "\n",
    "register_video_model(\n",
    "    model_name=\"tiny_video_cnn\",\n",
    "    dim=feature_dim,\n",
    "    model=backbone,      # pass the trained backbone instance\n",
    "    clip_len=clip_len,\n",
    "    input_size=frame_size,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Use DLFeatExtractor to get features for a list of video paths\n",
    "# ---------------------------\n",
    "\n",
    "extractor = DLFeatExtractor(\"tiny_video_cnn\", device=device)\n",
    "\n",
    "# Here we just reuse a few validation paths, but any list of .mp4 files works\n",
    "video_paths = val_paths[:4]\n",
    "features = extractor.transform(video_paths, batch_size=2)\n",
    "\n",
    "print(\"Extracted feature shape:\", features.shape)  # (N_videos, feature_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa4f90c",
   "metadata": {},
   "source": [
    "## Image custom model example\n",
    "\n",
    "In this example we define a tiny convolutional backbone for RGB images, train it for a few epochs on a small synthetic dataset, and then register it in DLFeat with `register_image_model`. Once registered, the trained backbone can be reused via `DLFeatExtractor` to obtain fixed-size feature vectors from any list of images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3022c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - train loss: 1.3659 - val acc: 0.500\n",
      "Epoch 2/3 - train loss: 1.2388 - val acc: 0.500\n",
      "Epoch 3/3 - train loss: 0.9011 - val acc: 0.609\n",
      "Extracted feature shape: (4, 64)\n"
     ]
    }
   ],
   "source": [
    "# Image custom model example: tiny CNN + DLFeat registration\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# ---------------------------\n",
    "# Hyperparameters\n",
    "# ---------------------------\n",
    "input_size   = 64   # H = W of input images\n",
    "feature_dim  = 64   # feature dimension exposed to DLFeat\n",
    "num_classes  = 4\n",
    "num_train    = 256\n",
    "num_val      = 64\n",
    "batch_size   = 32\n",
    "epochs       = 3\n",
    "device       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# This transform will be used BOTH for training and when registering the model\n",
    "image_transform_for_model = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Synthetic image dataset\n",
    "# ---------------------------\n",
    "\n",
    "class RandomSquaresDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple synthetic dataset:\n",
    "    - each sample is a dark background with a colored square\n",
    "    - the square color encodes the class (0..num_classes-1)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, image_size, num_classes, split=\"train\", transform=None):\n",
    "        self.num_samples = num_samples\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        # Separate RNG for train/val so they don't share the same sequence\n",
    "        seed = 42 if split == \"train\" else 123\n",
    "        self.rng = torch.Generator().manual_seed(seed)\n",
    "\n",
    "        # Fixed palette: one color per class\n",
    "        self.palette = [\n",
    "            (220, 60, 60),\n",
    "            (60, 220, 60),\n",
    "            (60, 60, 220),\n",
    "            (220, 180, 60),\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def _randint(self, low, high):\n",
    "        return int(torch.randint(low, high, (1,), generator=self.rng).item())\n",
    "\n",
    "    def _make_image(self, label):\n",
    "        img = Image.new(\"RGB\", (self.image_size, self.image_size), color=(10, 10, 10))\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        square_size = self.image_size // 3\n",
    "        x0 = self._randint(0, self.image_size - square_size)\n",
    "        y0 = self._randint(0, self.image_size - square_size)\n",
    "        x1 = x0 + square_size\n",
    "        y1 = y0 + square_size\n",
    "\n",
    "        color = self.palette[label % len(self.palette)]\n",
    "        draw.rectangle([x0, y0, x1, y1], fill=color)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Simple deterministic label: cycle through classes\n",
    "        label = idx % self.num_classes\n",
    "        img = self._make_image(label)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "train_ds = RandomSquaresDataset(\n",
    "    num_samples=num_train,\n",
    "    image_size=input_size,\n",
    "    num_classes=num_classes,\n",
    "    split=\"train\",\n",
    "    transform=image_transform_for_model,\n",
    ")\n",
    "val_ds = RandomSquaresDataset(\n",
    "    num_samples=num_val,\n",
    "    image_size=input_size,\n",
    "    num_classes=num_classes,\n",
    "    split=\"val\",\n",
    "    transform=image_transform_for_model,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Tiny CNN backbone + classifier head\n",
    "# ---------------------------\n",
    "\n",
    "class TinyImageBackbone(nn.Module):\n",
    "    \"\"\"Very small 2D CNN that maps (B, 3, H, W) to (B, feature_dim).\"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),                  # 16 x 32 x 32\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),                  # 32 x 16 x 16\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),         # 64 x 1 x 1\n",
    "        )\n",
    "        self.proj = nn.Linear(64, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)                      # [B, 64, 1, 1]\n",
    "        x = x.view(x.size(0), 64)             # [B, 64]\n",
    "        return self.proj(x)                   # [B, feature_dim]\n",
    "\n",
    "\n",
    "class TinyImageClassifier(nn.Module):\n",
    "    \"\"\"Backbone + linear classification head.\"\"\"\n",
    "    def __init__(self, backbone, num_classes, feature_dim):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        logits = self.head(feats)\n",
    "        return logits\n",
    "\n",
    "\n",
    "backbone = TinyImageBackbone(feature_dim=feature_dim)\n",
    "model = TinyImageClassifier(backbone, num_classes=num_classes, feature_dim=feature_dim).to(device)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Quick training loop\n",
    "# ---------------------------\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_ds)\n",
    "\n",
    "    # Simple validation accuracy (optional but nice to see)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(imgs)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.numel()\n",
    "    val_acc = correct / max(total, 1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - train loss: {avg_loss:.4f} - val acc: {val_acc:.3f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Register the trained backbone with DLFeat\n",
    "# ---------------------------\n",
    "\n",
    "register_image_model(\n",
    "    model_name=\"tiny_cnn_image\",\n",
    "    dim=feature_dim,\n",
    "    model=backbone,                 # pass the trained backbone instance\n",
    "    input_size=input_size,\n",
    "    image_transform=image_transform_for_model,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Use DLFeatExtractor to get features for arbitrary images\n",
    "# ---------------------------\n",
    "\n",
    "extractor = DLFeatExtractor(\"tiny_cnn_image\", device=device)\n",
    "\n",
    "# Create a few random PIL images and extract features\n",
    "def make_random_pil_image(size=input_size):\n",
    "    arr = torch.randint(0, 255, (size, size, 3), dtype=torch.uint8).numpy()\n",
    "    return Image.fromarray(arr)\n",
    "\n",
    "test_images = [make_random_pil_image() for _ in range(4)]\n",
    "features = extractor.transform(test_images, batch_size=2)\n",
    "\n",
    "print(\"Extracted feature shape:\", features.shape)  # (N_images, feature_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e46974",
   "metadata": {},
   "source": [
    "## Audio custom model example\n",
    "This section shows how to define, train, and register a tiny 1D CNN for raw audio waveforms using DLFeat.  \n",
    "We create a synthetic dataset of short sine-wave signals, train a lightweight classifier, then register the backbone with `register_audio_model` and use `DLFeatExtractor` to obtain fixed-size embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "020666e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - train loss: 1.0562 - acc: 0.707\n",
      "Epoch 2/5 - train loss: 0.9180 - acc: 0.920\n",
      "Epoch 3/5 - train loss: 0.6874 - acc: 1.000\n",
      "Epoch 4/5 - train loss: 0.4264 - acc: 1.000\n",
      "Epoch 5/5 - train loss: 0.2126 - acc: 1.000\n",
      "Extracted feature shape: (8, 64)\n"
     ]
    }
   ],
   "source": [
    "# Audio custom model example: tiny 1D CNN + DLFeat registration\n",
    "\n",
    "import math\n",
    "import types\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ---------------------------\n",
    "# Hyperparameters\n",
    "# ---------------------------\n",
    "sample_rate = 16_000          # Hz\n",
    "duration_s = 0.5              # seconds\n",
    "num_samples = int(sample_rate * duration_s)\n",
    "\n",
    "num_classes = 3\n",
    "feature_dim = 64\n",
    "\n",
    "num_train_signals = 300\n",
    "num_val_signals = 60\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Synthetic audio dataset\n",
    "#    (sine waves at different frequencies + noise)\n",
    "# ---------------------------\n",
    "\n",
    "class SineWaveDataset(Dataset):\n",
    "    \"\"\"Synthetic dataset: each class = sine wave with a different base frequency.\"\"\"\n",
    "    def __init__(self, n_samples, num_classes, sample_rate, num_samples):\n",
    "        self.n_samples = n_samples\n",
    "        self.num_classes = num_classes\n",
    "        self.sample_rate = sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        # Choose one frequency per class\n",
    "        self.class_freqs = torch.tensor([220.0, 440.0, 880.0])[:num_classes]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cls = idx % self.num_classes\n",
    "        freq = self.class_freqs[cls]\n",
    "\n",
    "        t = torch.linspace(0, duration_s, self.num_samples, dtype=torch.float32)\n",
    "        phase = 2 * math.pi * torch.rand(1).item()\n",
    "        amplitude = 0.5 + 0.5 * torch.rand(1).item()\n",
    "\n",
    "        clean = amplitude * torch.sin(2 * math.pi * freq * t + phase)\n",
    "        noise = 0.05 * torch.randn_like(clean)\n",
    "        waveform = clean + noise  # shape [T]\n",
    "\n",
    "        return waveform, cls\n",
    "\n",
    "train_ds = SineWaveDataset(num_train_signals, num_classes, sample_rate, num_samples)\n",
    "val_ds   = SineWaveDataset(num_val_signals,   num_classes, sample_rate, num_samples)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Tiny 1D CNN encoder + classifier\n",
    "# ---------------------------\n",
    "\n",
    "class TinyAudioEncoder(nn.Module):\n",
    "    \"\"\"Very small 1D CNN that maps (B, 1, T) to feature vectors of size feature_dim.\"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=9, padding=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(4),\n",
    "\n",
    "            nn.Conv1d(16, 32, kernel_size=9, padding=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(4),\n",
    "\n",
    "            nn.AdaptiveAvgPool1d(1)  # -> (B, 32, 1)\n",
    "        )\n",
    "        self.proj = nn.Linear(32, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, T)\n",
    "        x = self.net(x)          # (B, 32, 1)\n",
    "        x = x.squeeze(-1)        # (B, 32)\n",
    "        x = self.proj(x)         # (B, feature_dim)\n",
    "        return x\n",
    "\n",
    "class TinyAudioClassifier(nn.Module):\n",
    "    \"\"\"Encoder + linear classification head.\"\"\"\n",
    "    def __init__(self, encoder, feature_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.head = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, T)\n",
    "        feats = self.encoder(x)         # (B, feature_dim)\n",
    "        logits = self.head(feats)       # (B, num_classes)\n",
    "        return logits\n",
    "\n",
    "encoder = TinyAudioEncoder(feature_dim=feature_dim)\n",
    "classifier = TinyAudioClassifier(encoder, feature_dim, num_classes).to(device)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Quick training loop\n",
    "# ---------------------------\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    classifier.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for waveforms, labels in train_loader:\n",
    "        # waveforms: (B, T)\n",
    "        waveforms = waveforms.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Add channel dimension -> (B, 1, T)\n",
    "        waveforms = waveforms.unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = classifier(waveforms)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * waveforms.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_ds)\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - train loss: {avg_loss:.4f} - acc: {acc:.3f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Wrap encoder in a HF-style module for DLFeat\n",
    "#    (so that .transform() can use .last_hidden_state)\n",
    "# ---------------------------\n",
    "\n",
    "class HFStyleAudioWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps TinyAudioEncoder so that forward(input_values) returns an object\n",
    "    with a .last_hidden_state attribute, as expected by DLFeat's audio path.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        # input_values: (B, T)\n",
    "        x = input_values.unsqueeze(1)      # (B, 1, T)\n",
    "        feats = self.encoder(x)            # (B, feature_dim)\n",
    "        # DLFeat will do outputs.last_hidden_state.mean(dim=1),\n",
    "        # so we expose an extra \"sequence\" dimension of length 1.\n",
    "        return types.SimpleNamespace(last_hidden_state=feats.unsqueeze(1))  # (B, 1, D)\n",
    "\n",
    "audio_backbone = HFStyleAudioWrapper(encoder).to(device)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Custom audio preprocessing for DLFeat\n",
    "#    Here we assume inputs to DLFeatExtractor.transform() are 1D tensors.\n",
    "# ---------------------------\n",
    "\n",
    "def tiny_audio_preprocess(audio_input):\n",
    "    \"\"\"\n",
    "    audio_input: a 1D torch.Tensor (waveform).\n",
    "    Returns a dict of tensors like a HF processor would.\n",
    "    \"\"\"\n",
    "    if not isinstance(audio_input, torch.Tensor):\n",
    "        raise TypeError(\"This example expects a 1D torch.Tensor as input.\")\n",
    "    # shape -> (1, T) so that DLFeat builds batches correctly\n",
    "    waveform = audio_input.float().unsqueeze(0)\n",
    "    return {\"input_values\": waveform}\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Register the audio model in DLFeat\n",
    "# ---------------------------\n",
    "\n",
    "register_audio_model(\n",
    "    model_name=\"tiny_audio_cnn\",\n",
    "    dim=feature_dim,\n",
    "    model=audio_backbone,         # pre-trained backbone instance\n",
    "    sampling_rate=sample_rate,    # only for metadata; not used by our custom preprocess\n",
    "    audio_preprocess=tiny_audio_preprocess,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Use DLFeatExtractor to compute features\n",
    "# ---------------------------\n",
    "\n",
    "extractor = DLFeatExtractor(\"tiny_audio_cnn\", device=device)\n",
    "\n",
    "# Take a few validation waveforms\n",
    "audio_examples = [val_ds[i][0] for i in range(8)]  # list of 1D tensors\n",
    "\n",
    "features = extractor.transform(audio_examples, batch_size=4)\n",
    "print(\"Extracted feature shape:\", features.shape)   # (N_signals, feature_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96910df1",
   "metadata": {},
   "source": [
    "## Text custom model example\n",
    "This section shows how to define, train, and register a tiny text encoder using DLFeat.  \n",
    "We build a synthetic toy dataset, train a lightweight RNN-based classifier, then register the backbone with `register_text_model` and use `DLFeatExtractor` to obtain fixed-size embeddings from raw text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c876da04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - train loss: 1.1003 - acc: 0.290\n",
      "Epoch 2/5 - train loss: 1.1012 - acc: 0.333\n",
      "Epoch 3/5 - train loss: 1.0993 - acc: 0.333\n",
      "Epoch 4/5 - train loss: 1.0993 - acc: 0.333\n",
      "Epoch 5/5 - train loss: 1.1005 - acc: 0.333\n",
      "Validation texts: ['this is class zero sample', 'this is class one example', 'this is class two item', 'this is class zero sample', 'this is class one example', 'this is class two item']\n",
      "Extracted feature shape: (6, 32)\n"
     ]
    }
   ],
   "source": [
    "# Text custom model example: tiny RNN encoder + DLFeat registration\n",
    "\n",
    "import types\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ---------------------------\n",
    "# Hyperparameters\n",
    "# ---------------------------\n",
    "num_classes = 3\n",
    "feature_dim = 32       # output embedding dimension for DLFeat\n",
    "embed_dim = 32\n",
    "hidden_dim = 32\n",
    "\n",
    "num_train_samples = 300\n",
    "num_val_samples = 60\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "max_len = 8            # fixed max sequence length for tokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Tiny toy text dataset\n",
    "#    (3 classes, each with a simple template sentence)\n",
    "# ---------------------------\n",
    "\n",
    "class TinyTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Toy dataset:\n",
    "      class 0 -> \"this is class zero sample\"\n",
    "      class 1 -> \"this is class one example\"\n",
    "      class 2 -> \"this is class two item\"\n",
    "    \"\"\"\n",
    "    def __init__(self, n_samples, num_classes=3):\n",
    "        assert num_classes <= 3, \"This toy dataset supports up to 3 classes.\"\n",
    "        self.n_samples = n_samples\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cls = idx % self.num_classes\n",
    "        if cls == 0:\n",
    "            text = \"this is class zero sample\"\n",
    "        elif cls == 1:\n",
    "            text = \"this is class one example\"\n",
    "        else:\n",
    "            text = \"this is class two item\"\n",
    "        return text, cls\n",
    "\n",
    "train_ds = TinyTextDataset(num_train_samples, num_classes=num_classes)\n",
    "val_ds   = TinyTextDataset(num_val_samples,   num_classes=num_classes)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Simple vocabulary + tokenizer\n",
    "# ---------------------------\n",
    "\n",
    "VOCAB_LIST = [\n",
    "    \"[PAD]\", \"[UNK]\",\n",
    "    \"this\", \"is\", \"class\", \"zero\", \"one\", \"two\",\n",
    "    \"sample\", \"example\", \"item\"\n",
    "]\n",
    "VOCAB = {w: i for i, w in enumerate(VOCAB_LIST)}\n",
    "PAD_ID = VOCAB[\"[PAD]\"]\n",
    "UNK_ID = VOCAB[\"[UNK]\"]\n",
    "\n",
    "def tiny_tokenizer(\n",
    "    texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=None,\n",
    "    max_length=max_len,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Minimal tokenizer with a HuggingFace-like signature.\n",
    "    - texts: str or list[str]\n",
    "    Returns:\n",
    "      {\n",
    "        \"input_ids\": LongTensor [B, L],\n",
    "        \"attention_mask\": LongTensor [B, L]\n",
    "      }\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    batch_ids = []\n",
    "    for t in texts:\n",
    "        tokens = t.lower().split()\n",
    "        ids = [VOCAB.get(tok, UNK_ID) for tok in tokens]\n",
    "\n",
    "        if truncation and len(ids) > max_length:\n",
    "            ids = ids[:max_length]\n",
    "\n",
    "        if padding:\n",
    "            while len(ids) < max_length:\n",
    "                ids.append(PAD_ID)\n",
    "\n",
    "        batch_ids.append(ids)\n",
    "\n",
    "    input_ids = torch.tensor(batch_ids, dtype=torch.long)\n",
    "    attention_mask = (input_ids != PAD_ID).long()\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "vocab_size = len(VOCAB_LIST)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Tiny RNN text backbone + classifier\n",
    "# ---------------------------\n",
    "\n",
    "class TinyTextBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    Tiny encoder that mimics the HF output interface:\n",
    "    forward(input_ids, attention_mask=None) -> object with .last_hidden_state\n",
    "    where last_hidden_state has shape [B, L, D].\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, feature_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.proj = nn.Linear(hidden_dim, feature_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # input_ids: [B, L]\n",
    "        x = self.embedding(input_ids)      # [B, L, E]\n",
    "        # simple GRU\n",
    "        output, _ = self.rnn(x)            # [B, L, H]\n",
    "        feats = self.proj(output)          # [B, L, D]\n",
    "        # DLFeat expects .last_hidden_state\n",
    "        return types.SimpleNamespace(last_hidden_state=feats)\n",
    "\n",
    "class TinyTextClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Backbone + head for quick supervised training.\n",
    "    We use the representation at position 0 (like a [CLS] token).\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, feature_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_emb = outputs.last_hidden_state[:, 0, :]  # [B, D]\n",
    "        logits = self.head(cls_emb)                   # [B, num_classes]\n",
    "        return logits\n",
    "\n",
    "backbone = TinyTextBackbone(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    feature_dim=feature_dim,\n",
    "    pad_idx=PAD_ID,\n",
    ")\n",
    "classifier = TinyTextClassifier(backbone, feature_dim, num_classes).to(device)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Quick training loop\n",
    "# ---------------------------\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    classifier.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for texts, labels in train_loader:\n",
    "        labels = labels.to(device)\n",
    "        # tokenize list of strings\n",
    "        batch_tokens = tiny_tokenizer(texts, return_tensors=\"pt\")\n",
    "        input_ids = batch_tokens[\"input_ids\"].to(device)\n",
    "        attention_mask = batch_tokens[\"attention_mask\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * input_ids.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_ds)\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - train loss: {avg_loss:.4f} - acc: {acc:.3f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Register the trained backbone with DLFeat\n",
    "# ---------------------------\n",
    "\n",
    "register_text_model(\n",
    "    model_name=\"tiny_text_rnn\",\n",
    "    dim=feature_dim,\n",
    "    model=backbone,          # pre-trained backbone instance\n",
    "    tokenizer=tiny_tokenizer,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Use DLFeatExtractor to obtain text features\n",
    "# ---------------------------\n",
    "\n",
    "extractor = DLFeatExtractor(\"tiny_text_rnn\", device=device)\n",
    "\n",
    "# Take a few validation sentences\n",
    "val_texts = [val_ds[i][0] for i in range(6)]\n",
    "features = extractor.transform(val_texts, batch_size=3)\n",
    "\n",
    "print(\"Validation texts:\", val_texts)\n",
    "print(\"Extracted feature shape:\", features.shape)  # (N_texts, feature_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e4f94",
   "metadata": {},
   "source": [
    "## Multimodal image–text custom model example\n",
    "This section shows how to define, train, and register a tiny image–text encoder with DLFeat.  \n",
    "We build a synthetic toy dataset of colored squares with simple text descriptions, train a lightweight multimodal classifier, then register the backbone with `register_multimodal_image_text_model` and use `DLFeatExtractor` to obtain aligned image and text embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5badbb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - train loss: 1.0809 - acc: 0.410\n",
      "Epoch 2/5 - train loss: 0.9158 - acc: 0.933\n",
      "Epoch 3/5 - train loss: 0.6145 - acc: 1.000\n",
      "Epoch 4/5 - train loss: 0.2440 - acc: 1.000\n",
      "Epoch 5/5 - train loss: 0.0508 - acc: 1.000\n",
      "Number of eval pairs: 6\n",
      "Image features shape: (6, 64)\n",
      "Text  features shape: (6, 64)\n"
     ]
    }
   ],
   "source": [
    "# Multimodal image–text custom model example: tiny encoders + DLFeat registration\n",
    "\n",
    "import types\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# ---------------------------\n",
    "# Hyperparameters\n",
    "# ---------------------------\n",
    "num_classes  = 3               # e.g. red / green / blue\n",
    "feature_dim  = 64              # shared embedding dim for image & text\n",
    "img_size     = 64\n",
    "embed_dim    = 32\n",
    "hidden_dim   = 32\n",
    "max_len      = 5               # max tokens in text\n",
    "\n",
    "num_train_samples = 300\n",
    "num_val_samples   = 60\n",
    "\n",
    "batch_size = 32\n",
    "epochs     = 5\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Tiny synthetic image–text dataset\n",
    "# ---------------------------\n",
    "\n",
    "COLORS = {\n",
    "    0: (\"red\",   (220, 40, 40)),\n",
    "    1: (\"green\", (40, 180, 60)),\n",
    "    2: (\"blue\",  (40, 80, 220)),\n",
    "}\n",
    "\n",
    "class TinyImageTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each sample:\n",
    "      - image: solid colored square (red/green/blue)\n",
    "      - text:  \"a red square\", \"a green square\", etc.\n",
    "      - label: 0, 1, or 2\n",
    "    \"\"\"\n",
    "    def __init__(self, n_samples, num_classes=3, img_size=64):\n",
    "        assert num_classes <= 3\n",
    "        self.n_samples = n_samples\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def _make_image(self, color_rgb):\n",
    "        img = Image.new(\"RGB\", (self.img_size, self.img_size), color=color_rgb)\n",
    "        # Optional: draw a little border just to make it less trivial\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        draw.rectangle([4, 4, self.img_size - 5, self.img_size - 5], outline=(0, 0, 0), width=2)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cls = idx % self.num_classes\n",
    "        color_name, rgb = COLORS[cls]\n",
    "        img = self._make_image(rgb)\n",
    "        text = f\"a {color_name} square\"\n",
    "        return img, text, cls\n",
    "\n",
    "train_ds = TinyImageTextDataset(num_train_samples, num_classes=num_classes, img_size=img_size)\n",
    "val_ds   = TinyImageTextDataset(num_val_samples,   num_classes=num_classes, img_size=img_size)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Keep PIL images and strings as lists (no default tensor collation),\n",
    "    stack labels into a tensor.\n",
    "    \"\"\"\n",
    "    images, texts, labels = zip(*batch)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return list(images), list(texts), labels\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Tiny vocabulary & tokenizer (text side)\n",
    "# ---------------------------\n",
    "\n",
    "VOCAB_LIST = [\n",
    "    \"[PAD]\", \"[UNK]\",\n",
    "    \"a\", \"red\", \"green\", \"blue\", \"square\"\n",
    "]\n",
    "VOCAB = {w: i for i, w in enumerate(VOCAB_LIST)}\n",
    "PAD_ID = VOCAB[\"[PAD]\"]\n",
    "UNK_ID = VOCAB[\"[UNK]\"]\n",
    "vocab_size = len(VOCAB_LIST)\n",
    "\n",
    "def tiny_text_tokenizer(\n",
    "    texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=max_len,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Minimal tokenizer with HF-like signature.\n",
    "    Returns:\n",
    "      {\n",
    "        \"input_ids\": LongTensor [B, L],\n",
    "        \"attention_mask\": LongTensor [B, L]\n",
    "      }\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    batch_ids = []\n",
    "    for t in texts:\n",
    "        tokens = t.lower().split()\n",
    "        ids = [VOCAB.get(tok, UNK_ID) for tok in tokens]\n",
    "\n",
    "        if truncation and len(ids) > max_length:\n",
    "            ids = ids[:max_length]\n",
    "\n",
    "        if padding:\n",
    "            while len(ids) < max_length:\n",
    "                ids.append(PAD_ID)\n",
    "\n",
    "        batch_ids.append(ids)\n",
    "\n",
    "    input_ids = torch.tensor(batch_ids, dtype=torch.long)\n",
    "    attention_mask = (input_ids != PAD_ID).long()\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Image preprocessing transform\n",
    "# ---------------------------\n",
    "\n",
    "img_transform = T.Compose([\n",
    "    T.Resize((img_size, img_size)),\n",
    "    T.ToTensor(),   # [C, H, W] in [0,1]\n",
    "])\n",
    "\n",
    "def preprocess_images(pil_images):\n",
    "    tensors = [img_transform(img) for img in pil_images]  # list of [C,H,W]\n",
    "    return torch.stack(tensors, dim=0)                    # [B,C,H,W]\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Multimodal processor for DLFeat (image + text)\n",
    "# ---------------------------\n",
    "\n",
    "def tiny_multimodal_processor(\n",
    "    text,\n",
    "    images,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=max_len,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Processor with HF CLIP-like API:\n",
    "      - text: list[str]\n",
    "      - images: list[PIL.Image.Image]\n",
    "    Returns a dict with keys:\n",
    "      \"pixel_values\", \"input_ids\", \"attention_mask\"\n",
    "    \"\"\"\n",
    "    # Tokenize text\n",
    "    text_tokens = tiny_text_tokenizer(\n",
    "        text,\n",
    "        padding=padding,\n",
    "        truncation=truncation,\n",
    "        return_tensors=return_tensors,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    # Preprocess images\n",
    "    pixel_values = preprocess_images(images)  # [B,C,H,W]\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": text_tokens[\"input_ids\"],\n",
    "        \"attention_mask\": text_tokens[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Tiny image & text encoders + multimodal backbone\n",
    "# ---------------------------\n",
    "\n",
    "class TinyImageEncoder(nn.Module):\n",
    "    \"\"\"Simple CNN: (B,3,H,W) -> (B, feature_dim).\"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # H/2, W/2\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),  # [B, 32, 1, 1]\n",
    "        )\n",
    "        self.proj = nn.Linear(32, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)              # [B,32,1,1]\n",
    "        x = x.view(x.size(0), 32)    # [B,32]\n",
    "        return self.proj(x)          # [B,feature_dim]\n",
    "\n",
    "class TinyTextEncoder(nn.Module):\n",
    "    \"\"\"GRU text encoder: (B,L) -> (B, feature_dim).\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, feature_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.proj = nn.Linear(hidden_dim, feature_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embedding(input_ids)         # [B,L,E]\n",
    "        outputs, _ = self.rnn(x)              # [B,L,H]\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            mask = attention_mask.unsqueeze(-1).float()  # [B,L,1]\n",
    "            outputs = outputs * mask\n",
    "            lengths = mask.sum(dim=1).clamp(min=1.0)     # [B,1]\n",
    "            pooled = outputs.sum(dim=1) / lengths        # [B,H]\n",
    "        else:\n",
    "            pooled = outputs.mean(dim=1)\n",
    "\n",
    "        return self.proj(pooled)              # [B,feature_dim]\n",
    "\n",
    "class TinyImageTextBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    Multimodal backbone with HF-like output:\n",
    "      forward(pixel_values, input_ids, attention_mask=None)\n",
    "    -> object with .image_embeds and .text_embeds (both [B,feature_dim]).\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim, vocab_size, embed_dim, hidden_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.image_encoder = TinyImageEncoder(feature_dim)\n",
    "        self.text_encoder  = TinyTextEncoder(vocab_size, embed_dim, hidden_dim, feature_dim, pad_idx)\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask=None):\n",
    "        img_embeds  = self.image_encoder(pixel_values)\n",
    "        text_embeds = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return types.SimpleNamespace(\n",
    "            image_embeds=img_embeds,\n",
    "            text_embeds=text_embeds,\n",
    "        )\n",
    "\n",
    "class TinyImageTextClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Backbone + linear head for quick supervised training.\n",
    "    We simply average image and text embeddings and classify.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, feature_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask=None):\n",
    "        outputs = self.backbone(\n",
    "            pixel_values=pixel_values,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        fused = 0.5 * (outputs.image_embeds + outputs.text_embeds)  # [B,feature_dim]\n",
    "        logits = self.head(fused)\n",
    "        return logits\n",
    "\n",
    "backbone = TinyImageTextBackbone(\n",
    "    feature_dim=feature_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    pad_idx=PAD_ID,\n",
    ")\n",
    "classifier = TinyImageTextClassifier(backbone, feature_dim, num_classes).to(device)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Quick training loop\n",
    "# ---------------------------\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    classifier.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, texts, labels in train_loader:\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Preprocess with our multimodal processor\n",
    "        proc = tiny_multimodal_processor(text=texts, images=images, return_tensors=\"pt\")\n",
    "        pixel_values   = proc[\"pixel_values\"].to(device)\n",
    "        input_ids      = proc[\"input_ids\"].to(device)\n",
    "        attention_mask = proc[\"attention_mask\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = classifier(\n",
    "            pixel_values=pixel_values,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_ds)\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - train loss: {avg_loss:.4f} - acc: {acc:.3f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Register the trained multimodal backbone with DLFeat\n",
    "# ---------------------------\n",
    "\n",
    "register_multimodal_image_text_model(\n",
    "    model_name=\"tiny_image_text_encoder\",\n",
    "    dim=feature_dim,\n",
    "    model=backbone,                  # pre-trained backbone instance\n",
    "    processor=tiny_multimodal_processor,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Use DLFeatExtractor to get image & text features\n",
    "# ---------------------------\n",
    "\n",
    "extractor = DLFeatExtractor(\"tiny_image_text_encoder\", device=device)\n",
    "\n",
    "# Build a small list of (PIL.Image, text) pairs from the validation set\n",
    "eval_pairs = []\n",
    "for i in range(6):\n",
    "    img, text, _ = val_ds[i]\n",
    "    eval_pairs.append((img, text))\n",
    "\n",
    "features = extractor.transform(eval_pairs, batch_size=3)\n",
    "\n",
    "print(\"Number of eval pairs:\", len(eval_pairs))\n",
    "print(\"Image features shape:\", features[\"image_features\"].shape)  # (N, feature_dim)\n",
    "print(\"Text  features shape:\", features[\"text_features\"].shape)   # (N, feature_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba255f2d",
   "metadata": {},
   "source": [
    "## Multimodal video–text custom model example\n",
    "In this example we build a tiny multimodal encoder that jointly processes short video clips and text descriptions.  \n",
    "We train it on a synthetic dataset and then register the backbone with `register_multimodal_video_text_model` so that `DLFeatExtractor` can return aligned video and text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f5d900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manu/.local/lib/python3.14/site-packages/torchvision/io/_video_deprecation_warning.py:9: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - train loss: 1.0771\n",
      "Epoch 2/3 - train loss: 0.9520\n",
      "Epoch 3/3 - train loss: 0.6978\n",
      "Video features shape: (4, 64)\n",
      "Text  features shape: (4, 64)\n"
     ]
    }
   ],
   "source": [
    "# Multimodal video–text custom model example: tiny encoder + DLFeat registration\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# ---------------------------\n",
    "# Hyperparameters\n",
    "# ---------------------------\n",
    "num_frames   = 8      # frames per clip\n",
    "frame_size   = 64     # spatial resolution (H = W)\n",
    "feature_dim  = 64     # output feature dimension for DLFeat\n",
    "num_classes  = 3\n",
    "num_train    = 24\n",
    "num_val      = 6\n",
    "batch_size   = 4\n",
    "epochs       = 3\n",
    "device       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Create a tiny synthetic video–text dataset on disk\n",
    "# ---------------------------\n",
    "\n",
    "tmp_root = tempfile.mkdtemp(prefix=\"dlfeat_tiny_mv_\")\n",
    "\n",
    "COLORS = [\n",
    "    (\"red\",   (255, 64, 64)),\n",
    "    (\"green\", (64, 255, 64)),\n",
    "    (\"blue\",  (64, 64, 255)),\n",
    "]\n",
    "\n",
    "def make_colored_video(path, rgb, num_frames=num_frames, size=frame_size):\n",
    "    \"\"\"\n",
    "    Create a very simple RGB video: a solid-colored frame with a small moving square.\n",
    "    \"\"\"\n",
    "    video = torch.zeros(num_frames, size, size, 3, dtype=torch.uint8)  # [T, H, W, C]\n",
    "    base_color = torch.tensor(rgb, dtype=torch.uint8)\n",
    "\n",
    "    for t in range(num_frames):\n",
    "        video[t] = base_color\n",
    "        # small darker square moving diagonally\n",
    "        x0 = (t * 3) % (size - 10)\n",
    "        y0 = (t * 2) % (size - 10)\n",
    "        video[t, y0:y0+10, x0:x0+10, :] = base_color // 2\n",
    "\n",
    "    # Requires PyAV installed\n",
    "    torchvision.io.write_video(path, video, fps=8)\n",
    "\n",
    "def generate_split(n_samples, split_name):\n",
    "    paths, texts, labels = [], [], []\n",
    "    for i in range(n_samples):\n",
    "        cls = i % num_classes\n",
    "        color_name, rgb = COLORS[cls]\n",
    "        filename = os.path.join(tmp_root, f\"{split_name}_{i:03d}_class{cls}.mp4\")\n",
    "        make_colored_video(filename, rgb)\n",
    "        paths.append(filename)\n",
    "        texts.append(f\"a short video of a {color_name} moving square\")\n",
    "        labels.append(cls)\n",
    "    return paths, texts, torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "train_paths, train_texts, train_labels = generate_split(num_train, \"train\")\n",
    "val_paths,   val_texts,   val_labels   = generate_split(num_val,   \"val\")\n",
    "\n",
    "\n",
    "class VideoTextFileDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that stores (video_path, text, label).\n",
    "    We will decode frames inside the training loop via a helper.\n",
    "    \"\"\"\n",
    "    def __init__(self, paths, texts, labels):\n",
    "        self.paths = paths\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.paths[idx], self.texts[idx], int(self.labels[idx])\n",
    "\n",
    "\n",
    "def collate_video_text(batch):\n",
    "    video_paths, texts, labels = zip(*batch)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return list(video_paths), list(texts), labels\n",
    "\n",
    "\n",
    "train_ds = VideoTextFileDataset(train_paths, train_texts, train_labels)\n",
    "val_ds   = VideoTextFileDataset(val_paths,   val_texts,   val_labels)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_video_text,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_video_text,\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Tiny video–text encoders\n",
    "# ---------------------------\n",
    "\n",
    "class TinyVideoEncoder(nn.Module):\n",
    "    \"\"\"Very small 3D CNN: (B, C, T, H, W) -> (B, feature_dim).\"\"\"\n",
    "    def __init__(self, feature_dim=feature_dim):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d((1, 2, 2)),           # pool spatial dims\n",
    "            nn.Conv3d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool3d((None, 1, 1)),  # keep time, pool H,W\n",
    "        )\n",
    "        self.proj = nn.Linear(32, feature_dim)\n",
    "\n",
    "    def forward(self, videos):\n",
    "        # videos: [B, C, T, H, W]\n",
    "        x = self.features(videos)          # [B, 32, T, 1, 1]\n",
    "        x = x.mean(dim=2)                  # temporal average -> [B, 32, 1, 1]\n",
    "        x = x.view(x.size(0), 32)          # [B, 32]\n",
    "        return self.proj(x)                # [B, feature_dim]\n",
    "\n",
    "\n",
    "class TinyTextEncoder(nn.Module):\n",
    "    \"\"\"Simple GRU-based text encoder: token ids -> (B, feature_dim).\"\"\"\n",
    "    def __init__(self, vocab_size=128, emb_dim=64, hidden_dim=feature_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embedding(input_ids)  # [B, L, E]\n",
    "        outputs, _ = self.gru(x)\n",
    "        if attention_mask is not None:\n",
    "            attn = attention_mask.unsqueeze(-1).float()\n",
    "            summed = (outputs * attn).sum(dim=1)\n",
    "            lengths = attn.sum(dim=1).clamp(min=1.0)\n",
    "            return summed / lengths\n",
    "        else:\n",
    "            return outputs.mean(dim=1)\n",
    "\n",
    "\n",
    "class TinyVideoTextBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    Multimodal backbone returning two aligned embeddings:\n",
    "    - video_embeds: (B, feature_dim)\n",
    "    - text_embeds:  (B, feature_dim)\n",
    "    This is what DLFeatExtractor will call.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=feature_dim):\n",
    "        super().__init__()\n",
    "        self.video_encoder = TinyVideoEncoder(feature_dim=feature_dim)\n",
    "        self.text_encoder  = TinyTextEncoder(hidden_dim=feature_dim)\n",
    "\n",
    "    def forward(self, videos, input_ids, attention_mask=None):\n",
    "        # videos: [B, C, T, H, W]\n",
    "        v_emb = self.video_encoder(videos)\n",
    "        t_emb = self.text_encoder(input_ids, attention_mask)\n",
    "        return SimpleNamespace(video_embeds=v_emb, text_embeds=t_emb)\n",
    "\n",
    "\n",
    "class TinyVideoTextClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Backbone + linear head for quick supervised training.\n",
    "    We fuse video and text embeddings by simple averaging.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, videos, input_ids, attention_mask=None):\n",
    "        outs = self.backbone(videos=videos, input_ids=input_ids, attention_mask=attention_mask)\n",
    "        fused = 0.5 * (outs.video_embeds + outs.text_embeds)\n",
    "        return self.head(fused)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Tiny tokenizer + processor (DLFeat-style)\n",
    "# ---------------------------\n",
    "\n",
    "def tiny_text_tokenizer(texts, max_length=16):\n",
    "    \"\"\"\n",
    "    Toy tokenizer:\n",
    "    - lowercase\n",
    "    - keep only printable ASCII (32..126)\n",
    "    - map chars to ids in [1..127], 0 reserved for padding.\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    encoded = []\n",
    "    for t in texts:\n",
    "        t = t.lower()\n",
    "        ids = []\n",
    "        for ch in t:\n",
    "            if \" \" <= ch <= \"~\":\n",
    "                ids.append(ord(ch) - 31)  # 1..96-ish\n",
    "        ids = ids[:max_length]\n",
    "        if len(ids) < max_length:\n",
    "            ids = ids + [0] * (max_length - len(ids))\n",
    "        encoded.append(ids)\n",
    "\n",
    "    input_ids = torch.tensor(encoded, dtype=torch.long)\n",
    "    attention_mask = (input_ids != 0).long()\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "\n",
    "frame_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((frame_size, frame_size)),\n",
    "        transforms.ToTensor(),  # [C,H,W], in [0,1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "def tiny_multimodal_video_processor(text, videos, return_tensors=\"pt\", padding=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Processor compatible with DLFeatExtractor for 'multimodal_video_text'.\n",
    "\n",
    "    Expected input (from DLFeatExtractor):\n",
    "      - text: list of strings (one per video)\n",
    "      - videos: list of lists of PIL.Image (frames for each clip)\n",
    "    Returns a dict of tensors ready for the model(**inputs).\n",
    "    \"\"\"\n",
    "    # 1) Process videos -> [B, C, T, H, W]\n",
    "    video_tensors = []\n",
    "    for clip in videos:  # clip: list[PIL.Image]\n",
    "        frame_tensors = [frame_transform(f) for f in clip]           # list [C,H,W]\n",
    "        clip_tensor = torch.stack(frame_tensors, dim=1)              # [C,T,H,W]\n",
    "        video_tensors.append(clip_tensor)\n",
    "    videos_batch = torch.stack(video_tensors, dim=0)                 # [B,C,T,H,W]\n",
    "\n",
    "    # 2) Process text -> token ids + mask\n",
    "    text_tokens = tiny_text_tokenizer(text)\n",
    "    input_ids = text_tokens[\"input_ids\"]\n",
    "    attention_mask = text_tokens[\"attention_mask\"]\n",
    "\n",
    "    if return_tensors != \"pt\":\n",
    "        raise ValueError(\"This tiny processor only supports return_tensors='pt'.\")\n",
    "\n",
    "    return {\n",
    "        \"videos\": videos_batch,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Helper to read videos as lists of PIL frames for training\n",
    "# ---------------------------\n",
    "\n",
    "def read_video_as_pil_list(path, num_frames=num_frames):\n",
    "    \"\"\"\n",
    "    Decode a video file into a list of PIL frames, uniformly sampled.\n",
    "    \"\"\"\n",
    "    frames_tensor, _, _ = torchvision.io.read_video(path, pts_unit=\"sec\")\n",
    "    if frames_tensor.numel() == 0:\n",
    "        raise ValueError(f\"No frames found in {path}\")\n",
    "\n",
    "    total = frames_tensor.shape[0]\n",
    "    if total < num_frames:\n",
    "        pad = frames_tensor[-1:].repeat(num_frames - total, 1, 1, 1)\n",
    "        frames_tensor = torch.cat([frames_tensor, pad], dim=0)\n",
    "    else:\n",
    "        idx = torch.linspace(0, total - 1, steps=num_frames).long()\n",
    "        frames_tensor = frames_tensor[idx]\n",
    "\n",
    "    frames = []\n",
    "    for t in range(num_frames):\n",
    "        frame_np = frames_tensor[t].numpy()  # [H,W,C]\n",
    "        frames.append(Image.fromarray(frame_np))\n",
    "    return frames\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Quick supervised training on the synthetic dataset\n",
    "# ---------------------------\n",
    "\n",
    "backbone = TinyVideoTextBackbone(feature_dim=feature_dim)\n",
    "model = TinyVideoTextClassifier(backbone=backbone, num_classes=num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for video_paths, texts, labels in train_loader:\n",
    "        # Decode videos to lists of PIL frames\n",
    "        batch_videos = [read_video_as_pil_list(p) for p in video_paths]\n",
    "\n",
    "        # Use the same processor we will give to DLFeat\n",
    "        proc_out = tiny_multimodal_video_processor(text=texts, videos=batch_videos, return_tensors=\"pt\")\n",
    "        videos_tensor = proc_out[\"videos\"].to(device)\n",
    "        input_ids     = proc_out[\"input_ids\"].to(device)\n",
    "        attention_mask = proc_out[\"attention_mask\"].to(device)\n",
    "\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(videos=videos_tensor, input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_ds)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - train loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Register the trained backbone with DLFeat\n",
    "# ---------------------------\n",
    "\n",
    "register_multimodal_video_text_model(\n",
    "    model_name=\"xclip_tiny_video_text\",   # prefix 'xclip' so DLFeat uses frame lists\n",
    "    dim=feature_dim,\n",
    "    model=backbone,                       # we register only the backbone (features)\n",
    "    processor=tiny_multimodal_video_processor,\n",
    "    num_frames=num_frames,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Use DLFeatExtractor to get aligned video/text features\n",
    "# ---------------------------\n",
    "\n",
    "extractor = DLFeatExtractor(\"xclip_tiny_video_text\", device=device) # type: ignore\n",
    "\n",
    "# Build a small list of (video_path, text) pairs (could be any external data)\n",
    "eval_pairs = list(zip(val_paths[:4], val_texts[:4]))\n",
    "features_dict = extractor.transform(eval_pairs, batch_size=2)\n",
    "\n",
    "print(\"Video features shape:\", features_dict[\"video_features\"].shape)\n",
    "print(\"Text  features shape:\", features_dict[\"text_features\"].shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
